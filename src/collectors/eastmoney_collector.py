import requests
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
import hashlib


class EastMoneyCollector:
    """ä¸œæ–¹è´¢å¯Œå¿«è®¯é‡‡é›†å™¨ï¼ˆä½¿ç”¨çœŸå®APIï¼‰"""

    def __init__(self):
        # æ‚¨æ‰¾åˆ°çš„çœŸå®APIåœ°å€
        self.base_url = "https://np-weblist.eastmoney.com/comm/web/getFastNewsList"

        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://kuaixun.eastmoney.com/',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }

        # æ ¹æ®æ‚¨æˆªå›¾ä¸­çš„å‚æ•°æ„å»º
        self.base_params = {
            'client': 'web',
            'biz': 'web_724',
            'fastColumn': '102',  # å¿«è®¯æ ç›®ID
            'pageSize': 20,  # æ¯é¡µæ¡æ•°
            'sortEnd': int(time.time() * 1000000),  # å¾®ç§’æ—¶é—´æˆ³
            'req_trace': int(time.time() * 1000),  # æ¯«ç§’æ—¶é—´æˆ³
            '_': int(time.time() * 1000),
            'callback': f'jQuery_{int(time.time() * 1000)}'
        }

    def fetch_news(self, page_size: int = 20) -> Optional[List[Dict]]:
        """
        è·å–ä¸œæ–¹è´¢å¯Œå¿«è®¯æ–°é—»

        Args:
            page_size: æ¯é¡µæ•°é‡

        Returns:
            ç»“æ„åŒ–çš„æ–°é—»åˆ—è¡¨
        """
        try:
            # æ›´æ–°å‚æ•°
            params = self.base_params.copy()
            params['pageSize'] = page_size
            params['sortEnd'] = int(time.time() * 1000000)
            params['_'] = int(time.time() * 1000)
            params['callback'] = f'jQuery_{int(time.time() * 1000)}'

            print(f"æ­£åœ¨æŠ“å–å¿«è®¯ï¼Œæ¯é¡µ {page_size} æ¡...")
            print(f"API URL: {self.base_url}")

            response = requests.get(
                self.base_url,
                params=params,
                headers=self.headers,
                timeout=15
            )

            response.raise_for_status()
            print(f"HTTPçŠ¶æ€ç : {response.status_code}")

            # å¤„ç†JSONPå“åº”
            raw_text = response.text
            print(f"åŸå§‹å“åº”é•¿åº¦: {len(raw_text)} å­—ç¬¦")

            # æå–JSONéƒ¨åˆ†ï¼ˆJSONPæ ¼å¼ï¼šcallback({...})ï¼‰
            json_start = raw_text.find('(')
            json_end = raw_text.rfind(')')

            if json_start != -1 and json_end != -1:
                json_str = raw_text[json_start + 1:json_end]
                data = json.loads(json_str)
                print(f"æˆåŠŸè§£æJSONæ•°æ®")

                # è§£ææ–°é—»æ•°æ®
                news_list = self._parse_news_data(data)
                return news_list
            else:
                print(f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONPæ ¼å¼")
                print(f"å“åº”é¢„è§ˆ: {raw_text[:200]}...")
                return None

        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            print(f"åŸå§‹å“åº”: {raw_text[:500]}...")
            return None
        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _parse_news_data(self, data: Dict) -> List[Dict]:
        """
        è§£æAPIè¿”å›çš„æ–°é—»æ•°æ®

        æ ¹æ®ä¸œæ–¹è´¢å¯ŒAPIçš„å®é™…æ•°æ®ç»“æ„è¿›è¡Œè§£æ
        """
        news_items = []

        print(f"APIè¿”å›æ•°æ®é”®å: {list(data.keys())}")

        # æ ¹æ®å¸¸è§çš„APIç»“æ„æŸ¥æ‰¾æ–°é—»æ•°æ®
        # å¯èƒ½çš„æ•°æ®ç»“æ„ï¼šdataå­—æ®µã€resultå­—æ®µæˆ–ç›´æ¥æ˜¯æ•°ç»„
        if isinstance(data, dict):
            # å°è¯•ä¸åŒçš„æ•°æ®ä½ç½®
            data_locations = ['data', 'result', 'list', 'news']

            news_data = None
            for location in data_locations:
                if location in data:
                    news_data = data[location]
                    print(f"æ‰¾åˆ°æ–°é—»æ•°æ®åœ¨ '{location}' å­—æ®µ")
                    break

            # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šå­—æ®µï¼Œå°è¯•dataæœ¬èº«
            if news_data is None:
                news_data = data
        elif isinstance(data, list):
            news_data = data
            print(f"APIç›´æ¥è¿”å›åˆ—è¡¨ï¼Œé•¿åº¦: {len(news_data)}")
        else:
            print(f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {type(data)}")
            return news_items

        # å¤„ç†æ–°é—»æ•°æ®
        if isinstance(news_data, list):
            print(f"å¼€å§‹è§£æ {len(news_data)} æ¡æ–°é—»...")

            for i, item in enumerate(news_data):
                try:
                    # è§£æå•æ¡æ–°é—»
                    news_item = self._parse_single_news(item)
                    if news_item and news_item.get('title'):
                        news_items.append(news_item)

                        # åªæ˜¾ç¤ºå‰3æ¡ä½œä¸ºç¤ºä¾‹
                        if i < 3:
                            print(f"  ç¤ºä¾‹{i + 1}: {news_item['title'][:50]}...")

                except Exception as e:
                    print(f"è§£æç¬¬{i + 1}æ¡æ–°é—»å¤±è´¥: {e}")
                    continue

        elif isinstance(news_data, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼Œå¯èƒ½åŒ…å«åˆ†é¡µä¿¡æ¯
            print(f"æ–°é—»æ•°æ®æ˜¯å­—å…¸ï¼Œé”®å: {list(news_data.keys())}")

            # å°è¯•åœ¨å­—å…¸ä¸­æŸ¥æ‰¾åˆ—è¡¨
            for key, value in news_data.items():
                if isinstance(value, list):
                    print(f"åœ¨ '{key}' ä¸­æ‰¾åˆ°åˆ—è¡¨æ•°æ®ï¼Œé•¿åº¦: {len(value)}")
                    news_items.extend([self._parse_single_news(item) for item in value[:10]])
                    break
        else:
            print(f"æ— æ³•å¤„ç†çš„æ–°é—»æ•°æ®ç±»å‹: {type(news_data)}")

        return news_items

    def _parse_single_news(self, item) -> Dict:
        """è§£æå•æ¡æ–°é—»"""
        try:
            # ä¸ºæ–°é—»ç”Ÿæˆå”¯ä¸€ID
            news_id = hashlib.md5(str(item).encode()).hexdigest()[:16]

            # æ ¹æ®å¸¸è§çš„å­—æ®µåæå–ä¿¡æ¯
            news_item = {
                'id': news_id,
                'raw_data': item  # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
            }

            # å°è¯•æå–æ ‡å‡†å­—æ®µï¼ˆæ ¹æ®ä¸œæ–¹è´¢å¯Œçš„å®é™…å­—æ®µåï¼‰
            field_mapping = {
                'title': ['title', 'Title', 'tit', 'newstitle'],
                'content': ['content', 'Content', 'body', 'newscontent', 'digest'],
                'time': ['time', 'Time', 'publish_time', 'showtime', 'ctime', 'timestamp'],
                'source': ['source', 'Source', 'media', 'author'],
                'url': ['url', 'Url', 'link', 'newsurl'],
                'category': ['category', 'Category', 'type', 'column'],
                'importance': ['importance', 'level', 'rank', 'hot']
            }

            # è‡ªåŠ¨åŒ¹é…å­—æ®µ
            if isinstance(item, dict):
                for field_name, possible_keys in field_mapping.items():
                    for key in possible_keys:
                        if key in item and item[key] is not None:
                            news_item[field_name] = str(item[key])
                            break
                    if field_name not in news_item:
                        news_item[field_name] = ''

            # ç¡®ä¿å¿…è¦å­—æ®µ
            news_item.setdefault('title', '')
            news_item.setdefault('content', '')
            news_item.setdefault('time', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
            news_item.setdefault('source', 'ä¸œæ–¹è´¢å¯Œ')
            news_item.setdefault('url', '')
            news_item.setdefault('category', '')
            news_item.setdefault('importance', 0)

            # æ¸…ç†æ•°æ®
            news_item['title'] = news_item['title'].strip()
            news_item['content'] = news_item['content'].strip()

            # ç”Ÿæˆæ‘˜è¦ï¼ˆå¦‚æœæ²¡æœ‰å†…å®¹åˆ™ç”¨æ ‡é¢˜ï¼‰
            if not news_item['content'] and news_item['title']:
                news_item['content'] = news_item['title']

            return news_item

        except Exception as e:
            print(f"è§£æå•æ¡æ–°é—»å¼‚å¸¸: {e}")
            return {'title': 'è§£æå¤±è´¥', 'content': str(item)[:100]}

    def test_collection(self):
        """æµ‹è¯•é‡‡é›†åŠŸèƒ½"""
        print("=" * 60)
        print("ä¸œæ–¹è´¢å¯Œå¿«è®¯é‡‡é›†å™¨æµ‹è¯•ï¼ˆä½¿ç”¨çœŸå®APIï¼‰")
        print("=" * 60)

        # å°è¯•æŠ“å–ä¸åŒæ•°é‡çš„æ–°é—»è¿›è¡Œæµ‹è¯•
        for page_size in [5, 10, 20]:
            print(f"\nå°è¯•æŠ“å– {page_size} æ¡æ–°é—»...")
            news_list = self.fetch_news(page_size=page_size)

            if news_list:
                print(f"âœ… æˆåŠŸé‡‡é›†åˆ° {len(news_list)} æ¡æ–°é—»!")
                print("-" * 50)

                # æ˜¾ç¤ºæ‰€æœ‰æ–°é—»æ ‡é¢˜
                for i, news in enumerate(news_list[:10], 1):
                    time_str = news.get('time', 'N/A')
                    title = news.get('title', 'æ— æ ‡é¢˜')[:60]
                    source = news.get('source', 'N/A')
                    print(f"{i:2d}. [{time_str}] {title}... (æ¥æº: {source})")

                if len(news_list) > 10:
                    print(f"... è¿˜æœ‰ {len(news_list) - 10} æ¡æœªæ˜¾ç¤º")

                # ä¿å­˜è¯¦ç»†æ•°æ®ç”¨äºåˆ†æ
                self._save_debug_data(news_list)

                # éªŒè¯æ•°æ®è´¨é‡
                self._validate_data(news_list)
                return True
            else:
                print(f"âŒ é‡‡é›† {page_size} æ¡å¤±è´¥ï¼Œå°è¯•è°ƒæ•´å‚æ•°...")

        print("\næ‰€æœ‰å°è¯•å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIå‚æ•°")
        return False

    def _save_debug_data(self, news_list):
        """ä¿å­˜è°ƒè¯•æ•°æ®"""
        if news_list:
            # ä¿å­˜ç¬¬ä¸€æ¡æ–°é—»çš„å®Œæ•´æ•°æ®
            debug_data = {
                'total_count': len(news_list),
                'sample_news': news_list[0] if news_list else {},
                'all_titles': [news.get('title', '') for news in news_list]
            }

            with open('debug_eastmoney_news.json', 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, ensure_ascii=False, indent=2)

            print(f"\nè°ƒè¯•æ•°æ®å·²ä¿å­˜åˆ°: debug_eastmoney_news.json")

            # ä¹Ÿä¿å­˜åŸå§‹å“åº”ç”¨äºåˆ†æ
            if news_list and 'raw_data' in news_list[0]:
                with open('debug_raw_response.json', 'w', encoding='utf-8') as f:
                    json.dump(news_list[0]['raw_data'], f, ensure_ascii=False, indent=2)
                print(f"åŸå§‹å“åº”æ•°æ®å·²ä¿å­˜åˆ°: debug_raw_response.json")

    def _validate_data(self, news_list):
        """éªŒè¯æ•°æ®è´¨é‡"""
        print("\næ•°æ®è´¨é‡æ£€æŸ¥:")
        print("-" * 30)

        total = len(news_list)
        if total == 0:
            print("âŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ–°é—»")
            return

        # ç»Ÿè®¡å­—æ®µå®Œæ•´æ€§
        fields = ['title', 'content', 'time', 'source']
        stats = {}

        for field in fields:
            count = sum(1 for news in news_list if news.get(field))
            stats[field] = count

        print(f"æ–°é—»æ€»æ•°: {total}")
        for field, count in stats.items():
            percentage = (count / total) * 100
            status = "âœ…" if percentage > 80 else "âš ï¸" if percentage > 50 else "âŒ"
            print(f"{status} {field}: {count}/{total} ({percentage:.1f}%)")

        # æ£€æŸ¥æ ‡é¢˜é•¿åº¦
        avg_title_len = sum(len(news.get('title', '')) for news in news_list) / total
        print(f"å¹³å‡æ ‡é¢˜é•¿åº¦: {avg_title_len:.1f} å­—ç¬¦")

        if avg_title_len < 5:
            print("âš ï¸ è­¦å‘Š: å¹³å‡æ ‡é¢˜é•¿åº¦è¿‡çŸ­ï¼Œå¯èƒ½æ•°æ®è§£ææœ‰è¯¯")


# ä¸»å‡½æ•° - ç›´æ¥è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    print("ä¸œæ–¹è´¢å¯Œå¿«è®¯é‡‡é›†å™¨ v2.0")
    print("åŸºäºçœŸå®API: https://np-weblist.eastmoney.com/comm/web/getFastNewsList")
    print()

    collector = EastMoneyCollector()
    success = collector.test_collection()

    print("\n" + "=" * 60)
    if success:
        print("âœ… é‡‡é›†å™¨æµ‹è¯•æˆåŠŸï¼")
        print("\nğŸ‰ æ­å–œï¼æ‚¨å·²æˆåŠŸå®Œæˆï¼š")
        print("1. âœ… æ‰¾åˆ°ä¸œæ–¹è´¢å¯ŒçœŸå®å¿«è®¯API")
        print("2. âœ… å®ç°å¯å·¥ä½œçš„é‡‡é›†å™¨")
        print("3. âœ… è·å–ç»“æ„åŒ–æ–°é—»æ•°æ®")

        print("\nğŸ“‹ ä¸‹ä¸€æ­¥è®¡åˆ’ï¼ˆM1.1 å®Œæˆåçš„åç»­æ­¥éª¤ï¼‰:")
        print("1. é›†æˆè°ƒåº¦å™¨ï¼ˆAPSchedulerï¼‰å®ç°å®šæ—¶é‡‡é›†")
        print("2. è®¾è®¡æ•°æ®åº“è¡¨ç»“æ„å¹¶å®ç°å­˜å‚¨")
        print("3. æ·»åŠ åŸºç¡€å»é‡åŠŸèƒ½ï¼ˆURLå“ˆå¸Œï¼‰")
        print("4. åˆ›å»ºç®€å•çš„å‘½ä»¤è¡Œç®¡ç†ç•Œé¢")
    else:
        print("âŒ é‡‡é›†å™¨æµ‹è¯•å¤±è´¥")
        print("\nğŸ”§ è°ƒè¯•å»ºè®®:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. æ£€æŸ¥APIå‚æ•°æ˜¯å¦è¿‡æœŸ")
        print("3. æŸ¥çœ‹ç”Ÿæˆçš„è°ƒè¯•æ–‡ä»¶åˆ†ææ•°æ®ç»“æ„")
        print("4. å°è¯•åœ¨æµè§ˆå™¨ä¸­ç›´æ¥è®¿é—®APIé“¾æ¥æµ‹è¯•")