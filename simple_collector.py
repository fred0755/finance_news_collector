# simple_collector.py
"""
ç®€åŒ–çš„æ–°é—»é‡‡é›†å™¨
"""

import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict
import json
import os


def test_rss_sources():
    """æµ‹è¯•RSSæº"""
    test_sources = [
        ("BBCä¸­æ–‡è´¢ç»", "https://www.bbc.com/zhongwen/simp/business/index.xml"),
        ("Reutersä¸­æ–‡è´¢ç»", "https://cn.reuters.com/rssFeed/CNTopGenNews/"),
        ("FTä¸­æ–‡ç½‘", "https://www.ftchinese.com/rss/news"),
        ("æ–°æµªè´¢ç»å›½é™…", "https://finance.sina.com.cn/7x24/rssdomestic.xml"),
        ("åå°”è¡—æ—¥æŠ¥ä¸­æ–‡ç‰ˆ", "https://cn.wsj.com/rss/CN.xml"),
        ("ç½‘æ˜“è´¢ç»", "https://www.163.com/rss/0101.xml"),
        ("æœç‹è´¢ç»", "https://rss.sohu.com/rss/finance.xml"),
    ]

    print("æµ‹è¯•RSSæº...")
    working_sources = []

    for name, url in test_sources:
        print(f"\næµ‹è¯• {name}: {url}")
        try:
            response = requests.get(url, timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })

            print(f"  çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                feed = feedparser.parse(url)
                print(f"  æ¡ç›®æ•°: {len(feed.entries)}")

                if feed.entries:
                    working_sources.append((name, url))
                    print(f"  âœ“ å¯ç”¨")

                    # æ˜¾ç¤ºç¬¬ä¸€æ¡æ–°é—»
                    if feed.entries:
                        entry = feed.entries[0]
                        print(f"  ç¤ºä¾‹: {entry.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                else:
                    print(f"  âœ— æ— å†…å®¹")
            else:
                print(f"  âœ— HTTPé”™è¯¯")

        except Exception as e:
            print(f"  âœ— é”™è¯¯: {e}")

    return working_sources


def collect_news_from_rss(rss_url: str, source_name: str, max_items: int = 10) -> List[Dict]:
    """ä»RSSæºæ”¶é›†æ–°é—»"""
    news_items = []

    try:
        feed = feedparser.parse(rss_url)

        # åªè·å–æœ€è¿‘2å¤©çš„æ–°é—»
        cutoff_date = datetime.now() - timedelta(days=2)

        for entry in feed.entries[:max_items]:
            # è·å–å‘å¸ƒæ—¥æœŸ
            pub_date_str = entry.get('published', entry.get('updated', ''))

            # è§£ææ—¥æœŸ
            try:
                import dateutil.parser
                pub_date = dateutil.parser.parse(pub_date_str)
            except:
                pub_date = datetime.now()

            # åªæ”¶é›†æœ€è¿‘2å¤©çš„æ–°é—»
            if pub_date >= cutoff_date:
                news_item = {
                    'source': source_name,
                    'title': entry.get('title', ''),
                    'summary': entry.get('summary', entry.get('description', '')),
                    'link': entry.get('link', ''),
                    'publish_date': pub_date.strftime('%Y-%m-%d %H:%M:%S'),
                    'keywords': extract_keywords(entry.get('title', '')),
                    'crawl_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                news_items.append(news_item)

    except Exception as e:
        print(f"ä» {source_name} æ”¶é›†æ–°é—»å¤±è´¥: {e}")

    return news_items


def extract_keywords(text: str) -> List[str]:
    """æå–å…³é”®è¯"""
    if not text:
        return []

    keywords = [
        'Aè‚¡', 'æ¸¯è‚¡', 'ç¾è‚¡', 'ç§‘åˆ›æ¿', 'åˆ›ä¸šæ¿',
        'è¯ç›‘ä¼š', 'å¤®è¡Œ', 'ç¾è”å‚¨',
        'CPI', 'PPI', 'PMI', 'GDP',
        'åŠ æ¯', 'é™æ¯', 'åˆ©ç‡',
        'äººå·¥æ™ºèƒ½', 'æ–°èƒ½æº', 'åŠå¯¼ä½“', 'èŠ¯ç‰‡',
        'è´¢æŠ¥', 'ä¸šç»©', 'ç›ˆåˆ©',
        'ä¸Šæ¶¨', 'ä¸‹è·Œ', 'åå¼¹'
    ]

    found_keywords = []
    for keyword in keywords:
        if keyword in text:
            found_keywords.append(keyword)

    return found_keywords


def analyze_news(news_data: List[Dict]) -> Dict:
    """åˆ†ææ–°é—»æ•°æ®"""
    if not news_data:
        return {"error": "æ²¡æœ‰æ–°é—»æ•°æ®"}

    # åŸºç¡€ç»Ÿè®¡
    total_news = len(news_data)

    # æŒ‰æ¥æºç»Ÿè®¡
    by_source = {}
    for news in news_data:
        source = news['source']
        by_source[source] = by_source.get(source, 0) + 1

    # æå–æ‰€æœ‰å…³é”®è¯
    all_keywords = []
    for news in news_data:
        all_keywords.extend(news.get('keywords', []))

    # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
    from collections import Counter
    keyword_counter = Counter(all_keywords)
    top_keywords = dict(keyword_counter.most_common(10))

    return {
        'summary': {
            'total_news': total_news,
            'sources_count': len(by_source),
            'latest_collection': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        },
        'by_source': by_source,
        'top_keywords': top_keywords,
        'sample_news': news_data[:5]
    }


def save_results(news_data: List[Dict], analysis_result: Dict):
    """ä¿å­˜ç»“æœ"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs("data/raw", exist_ok=True)
    os.makedirs("data/reports", exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ä¿å­˜åŸå§‹æ•°æ®
    raw_file = f"data/raw/news_{timestamp}.json"
    with open(raw_file, 'w', encoding='utf-8') as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)
    print(f"åŸå§‹æ•°æ®ä¿å­˜åˆ°: {raw_file}")

    # ä¿å­˜åˆ†æç»“æœ
    analysis_file = f"data/reports/analysis_{timestamp}.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_result, f, ensure_ascii=False, indent=2)
    print(f"åˆ†æç»“æœä¿å­˜åˆ°: {analysis_file}")

    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    report_file = f"data/reports/report_{timestamp}.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("è´¢ç»æ–°é—»åˆ†ææŠ¥å‘Š\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {timestamp}\n")
        f.write("=" * 60 + "\n\n")

        f.write(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:\n")
        f.write(f"   æ–°é—»æ€»æ•°: {analysis_result['summary']['total_news']} æ¡\n")
        f.write(f"   æ¥æºåª’ä½“: {analysis_result['summary']['sources_count']} å®¶\n\n")

        f.write(f"ğŸ“° æ¥æºåˆ†å¸ƒ:\n")
        for source, count in analysis_result['by_source'].items():
            f.write(f"   {source}: {count} æ¡\n")
        f.write("\n")

        if analysis_result['top_keywords']:
            f.write(f"ğŸ”‘ çƒ­é—¨å…³é”®è¯:\n")
            for keyword, count in analysis_result['top_keywords'].items():
                f.write(f"   {keyword}: {count} æ¬¡\n")
            f.write("\n")

        f.write(f"ğŸ“‹ æ–°é—»æ‘˜è¦:\n")
        for i, news in enumerate(analysis_result['sample_news'], 1):
            f.write(f"\n{i}. [{news['source']}] {news['title']}\n")
            if news.get('summary'):
                f.write(f"   æ‘˜è¦: {news['summary'][:100]}...\n")
            if news.get('keywords'):
                f.write(f"   å…³é”®è¯: {', '.join(news['keywords'])}\n")

    print(f"æ–‡æœ¬æŠ¥å‘Šä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ç®€åŒ–ç‰ˆè´¢ç»æ–°é—»é‡‡é›†ç³»ç»Ÿ")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # 1. æµ‹è¯•RSSæº
    print("\n1. æµ‹è¯•RSSæº...")
    working_sources = test_rss_sources()

    if not working_sources:
        print("\nâš ï¸ æ²¡æœ‰å¯ç”¨çš„RSSæº")
        return

    print(f"\nâœ“ æ‰¾åˆ° {len(working_sources)} ä¸ªå¯ç”¨çš„RSSæº")

    # 2. æ”¶é›†æ–°é—»
    print("\n2. æ”¶é›†æ–°é—»...")
    all_news = []

    for name, url in working_sources:
        print(f"  ä» {name} æ”¶é›†æ–°é—»...")
        news_items = collect_news_from_rss(url, name)
        all_news.extend(news_items)
        print(f"    æ”¶é›†åˆ° {len(news_items)} æ¡æ–°é—»")

    if not all_news:
        print("\nâš ï¸ æœªæ”¶é›†åˆ°ä»»ä½•æ–°é—»")
        return

    print(f"\nâœ“ å…±æ”¶é›†åˆ° {len(all_news)} æ¡æ–°é—»")

    # 3. åˆ†ææ–°é—»
    print("\n3. åˆ†ææ–°é—»...")
    analysis_result = analyze_news(all_news)

    # 4. ä¿å­˜ç»“æœ
    print("\n4. ä¿å­˜ç»“æœ...")
    save_results(all_news, analysis_result)

    # 5. æ˜¾ç¤ºæ‘˜è¦
    print("\n" + "=" * 60)
    print("åˆ†æç»“æœæ‘˜è¦")
    print("=" * 60)

    print(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"   æ–°é—»æ€»æ•°: {analysis_result['summary']['total_news']} æ¡")
    print(f"   æ¥æºåª’ä½“: {analysis_result['summary']['sources_count']} å®¶")

    print(f"\nğŸ“° æ¥æºåˆ†å¸ƒ:")
    for source, count in analysis_result['by_source'].items():
        print(f"   {source}: {count} æ¡")

    if analysis_result['top_keywords']:
        print(f"\nğŸ”‘ çƒ­é—¨å…³é”®è¯:")
        for keyword, count in list(analysis_result['top_keywords'].items())[:5]:
            print(f"   {keyword}: {count} æ¬¡")

    print(f"\nğŸ“‹ æ–°é—»ç¤ºä¾‹:")
    for i, news in enumerate(analysis_result['sample_news'][:3], 1):
        print(f"\n{i}. [{news['source']}] {news['title']}")
        if news.get('summary'):
            print(f"   æ‘˜è¦: {news['summary'][:80]}...")

    print("\n" + "=" * 60)
    print("âœ… ç³»ç»Ÿè¿è¡ŒæˆåŠŸï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()